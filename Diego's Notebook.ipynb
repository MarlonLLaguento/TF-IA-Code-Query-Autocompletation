{"cells":[{"cell_type":"code","metadata":{"cell_id":"9ee778caede54f3fadc5526b5ce4b9b0","deepnote_cell_type":"code"},"source":"import os\nimport random\nimport string\n\ndef preprocess_query(query):\n    query = query.lower().strip()  # Convert to lowercase and remove leading/trailing spaces\n    query = \" \".join(query.split())  # Replace multiple adjacent spaces with a single space\n    return query\n\ndef read_aol_data(directory):\n    queries = []\n    for filename in os.listdir(directory):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(directory, filename)\n            with open(filepath, \"r\", encoding=\"latin-1\") as file:\n                for line in file:\n                    line = line.strip()\n                    if line:\n                        parts = line.split(\"\\t\")\n                        if len(parts) >= 2:\n                            query = preprocess_query(parts[1])\n                            queries.append(query)\n    return queries\n\ndef save_queries_to_file(queries, filename):\n    with open(filename, \"w\", encoding=\"utf-8\") as file:\n        for query in queries:\n            file.write(query + \"\\n\")\n\ndef split_dataset(queries, train_ratio, val_ratio):\n    random.shuffle(queries)\n    total_size = len(queries)\n    train_size = int(train_ratio * total_size)\n    val_size = int(val_ratio * total_size)\n    train_queries = queries[:train_size]\n    val_queries = queries[train_size:train_size+val_size]\n    test_queries = queries[train_size+val_size:]\n    return train_queries, val_queries, test_queries","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"64e5e999deb04982a195750c16d2a519","deepnote_cell_type":"code"},"source":"import torch\nimport torch.nn as nn\n\nclass QueryAutoCompletionModel(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size):\n        super(QueryAutoCompletionModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.lstm(embedded)\n        output = self.fc(output)\n        return output","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"6efb74deff8c44aa9f082b780a205655","deepnote_cell_type":"code"},"source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom data import read_aol_data, save_queries_to_file, split_dataset\nfrom model import QueryAutoCompletionModel\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\n\n# Define training parameters\ndata_directory = \"/work/aol_data\"\ntrain_file = \"train.txt\"\nval_file = \"val.txt\"\ntest_file = \"test.txt\"\nembedding_size = 100\nhidden_size = 200\nbatch_size = 64\nnum_epochs = 10\nlearning_rate = 0.001\n\n# Read AOL data and preprocess queries\nqueries = read_aol_data(data_directory)\n\n# Split dataset into train, validation, and test sets\ntrain_queries, val_queries, test_queries = split_dataset(queries, 0.7, 0.15)\n\n# Save queries to files\nsave_queries_to_file(train_queries, train_file)\nsave_queries_to_file(val_queries, val_file)\nsave_queries_to_file(test_queries, test_file)\n\n# Create a dictionary of tokens\nall_queries = train_queries + val_queries + test_queries\nvocab = sorted(set(\" \".join(all_queries).split()))\nword_to_idx = {word: idx+1 for idx, word in enumerate(vocab)}\nidx_to_word = {idx: word for word, idx in word_to_idx.items()}\n\n# Convert queries to numerical sequences\ntrain_sequences = [[word_to_idx[word] for word in query.split()] for query in train_queries]\nval_sequences = [[word_to_idx[word] for word in query.split()] for query in val_queries]\ntest_sequences = [[word_to_idx[word] for word in query.split()] for query in test_queries]\n\n# Pad sequences to have the same length\nmax_seq_length = max(max(len(seq) for seq in train_sequences),\n                     max(len(seq) for seq in val_sequences),\n                     max(len(seq) for seq in test_sequences))\ntrain_data = [seq + [0] * (max_seq_length - len(seq)) for seq in train_sequences]\nval_data = [seq + [0] * (max_seq_length - len(seq)) for seq in val_sequences]\ntest_data = [seq + [0] * (max_seq_length - len(seq)) for seq in test_sequences]\n\n# Convert data to PyTorch tensors\ntrain_tensor = torch.LongTensor(train_data)\nval_tensor = torch.LongTensor(val_data)\ntest_tensor = torch.LongTensor(test_data)\n\n# Create data loaders\ntrain_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_tensor, batch_size=batch_size)\ntest_loader = DataLoader(test_tensor, batch_size=batch_size)\n\n# Initialize the model\nvocab_size = len(vocab) + 1  # +1 for padding token\nmodel = QueryAutoCompletionModel(vocab_size, embedding_size, hidden_size)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        # Forward pass\n        output = model(batch)\n        loss = criterion(output[:, :-1].reshape(-1, vocab_size), batch[:, 1:].reshape(-1))\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    average_loss = total_loss / len(train_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n\n# Evaluation\nmodel.eval()\ntotal_loss = 0.0\n\nfor batch in val_loader:\n    with torch.no_grad():\n        output = model(batch)\n        loss = criterion(output[:, :-1].reshape(-1, vocab_size), batch[:, 1:].reshape(-1))\n        total_loss += loss.item()\n\naverage_loss = total_loss / len(val_loader)\nprint(f\"Validation Loss: {average_loss:.4f}\")\n\n# Save the best model\nbest_model_file = \"best_model.pt\"\ntorch.save(model.state_dict(), best_model_file)\n\n# Load the best model\nbest_model = QueryAutoCompletionModel(vocab_size, embedding_size, hidden_size)\nbest_model.load_state_dict(torch.load(best_model_file))\n\n# Evaluation on test set\nbest_model.eval()\ntotal_loss = 0.0\n\nfor batch in test_loader:\n    with torch.no_grad():\n        output = best_model(batch)\n        loss = criterion(output[:, :-1].reshape(-1, vocab_size), batch[:, 1:].reshape(-1))\n        total_loss += loss.item()\n\naverage_loss = total_loss / len(test_loader)\nperplexity = torch.exp(torch.tensor(average_loss))\nprint(f\"Test Loss: {average_loss:.4f}, Perplexity: {perplexity:.4f}\")","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"c2e7ce46","execution_start":1687737038036,"execution_millis":4229,"deepnote_to_be_reexecuted":false,"cell_id":"5ed90953623f40cb86cf7a5215eb087e","deepnote_cell_type":"code"},"source":"filename = \"/work/aol_data/user-ct-test-collection-01.txt\"\n\nif filename.endswith(\".txt\"):\n    with open(filename, \"r\") as file:\n        content = file.read()\n        print(\"The file is a text file.\")\n        print(\"File content:\")\n        print(content)\nelse:\n    print(\"The file is not a text file.\")\n","execution_count":6,"outputs":[{"text":"IOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_data_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b5db289b-3e20-4ae8-859e-975fa94ed9a8' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_full_width":false,"deepnote_notebook_id":"4c8bf205da8e48018f8be3ea8c67f614","deepnote_execution_queue":[]}}